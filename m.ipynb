{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "from kdtree import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefile = \"data.json\"\n",
    "points_count = 30000\n",
    "to_plot = False\n",
    "no_centres = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the parameters\n",
    "from sklearn.datasets import make_circles\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the parameters\n",
    "noise = 0.05  # Amount of noise\n",
    "\n",
    "# Generate the data\n",
    "X, y = make_circles(n_samples=points_count, noise=noise, factor=0.5, random_state=42)\n",
    "\n",
    "# Convert the data to a list of tuples\n",
    "points = [(x, y) for x, y in X]\n",
    "# points = [(round(x , 1), round(y , 1)) for x, y in X]\n",
    "maxdis = math.ceil(math.log2(points_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_blobs(n_samples=points_count, centers=no_centres, random_state=42)\n",
    "points = [(x, y) for x, y in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcran_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build():\n",
    "    tree = KDTree()\n",
    "    tree.root = tree.build(points)\n",
    "\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for point in points:\n",
    "        G.add_node(point , pos = point)\n",
    "\n",
    "    neighbours = {}\n",
    "    maxdis = math.ceil(math.log2(points_count))\n",
    "    for point in points:\n",
    "        neighbours[point] = i_neighbors(tree, point, maxdis)\n",
    "    return  G,  neighbours\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_comps(core1 , core2, core_points_map , mst):\n",
    "    pivot2 = min(core_points_map[core2], key=lambda node: euclidean_distance(node, core1))\n",
    "    pivot1 = min(core_points_map[core1], key=lambda node: euclidean_distance(node, pivot2))\n",
    "    mst.add_edge(pivot1, pivot2 , weight = euclidean_distance(pivot1, pivot2))\n",
    "    print(f\"merging {core1} and {core2} with pivot {pivot1} and {pivot2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_phase(G):\n",
    "    core_points_map = {}\n",
    "    for component in nx.connected_components(G):\n",
    "        centroid = np.mean([node for node in component], axis=0)\n",
    "        closest_point = min(component, key=lambda node: euclidean_distance(node, centroid))\n",
    "        core_points_map[closest_point] = component\n",
    "\n",
    "    core_points = list(core_points_map.keys())\n",
    "\n",
    "    minc = [float(\"inf\")] * len(core_points[0])\n",
    "    maxc = [float(\"-inf\")] * len(core_points[0])\n",
    "\n",
    "    for point in core_points:\n",
    "        for i in range(len(point)):\n",
    "            minc[i] = min(minc[i], point[i])\n",
    "            maxc[i] = max(maxc[i], point[i])\n",
    "\n",
    "    diff = [maxc[i] - minc[i] for i in range(len(minc))]\n",
    "    min_diff_axis = diff.index(max(diff))\n",
    "\n",
    "    sorted_core_points = sorted(core_points, key=lambda point: point[min_diff_axis])\n",
    "    for core1 , core2 in zip(sorted_core_points, sorted_core_points[1:]):\n",
    "        merge_comps(core1, core2, core_points_map, G)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcrun():\n",
    "    G,   neighbours = build()\n",
    "    print(G.number_of_nodes() , G.number_of_edges())\n",
    "    k = 0\n",
    "    while ( k**k   < maxdis) :\n",
    "\n",
    "        if(len(connected_components := list(nx.connected_components(G)))) == 1 : break\n",
    "        for component in connected_components:\n",
    "            for node in component:\n",
    "                wt , pos = neighbours[node][k]\n",
    "                # if pos in component : continue\n",
    "                G.add_edge(node , pos , weight = wt)\n",
    "                \n",
    "                wt, pos = neighbours[node][k**k]\n",
    "                # if pos in component:\n",
    "                    # continue\n",
    "                G.add_edge(node, pos, weight=wt)\n",
    "        k += 1\n",
    "    else :\n",
    "        print(\"merge phase\")\n",
    "        merge_phase(G)\n",
    "        print(\n",
    "            f\"The graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\"\n",
    "        )\n",
    "        print(\"Connected Components : \" , len(list(nx.connected_components(G))))\n",
    "    edgecount = G.number_of_edges()\n",
    "    mst = nx.minimum_spanning_tree(G)    \n",
    "    print(mst.number_of_nodes() , mst.number_of_edges())\n",
    "    mst_weight = sum(data[\"weight\"] for u, v, data in mst.edges(data=True)) \n",
    "    print(\n",
    "        f\"Minimum Spanning Tree: {mst.number_of_nodes()} nodes, {mst.number_of_edges()} edges, Total Weight: {mst_weight}\"\n",
    "    )\n",
    "    return mst_weight , edgecount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_weight, dc_edgecount = dcrun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcran_end_time = time.time()\n",
    "dcran_elapsed_time = dcran_end_time - dcran_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prim_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gst = nx.Graph()\n",
    "\n",
    "# for pointi in points:\n",
    "#     Gst.add_node(pointi, pos=pointi)\n",
    "    \n",
    "# for pointi in points:    \n",
    "#     for pointj in points:\n",
    "#         if pointi != pointj:\n",
    "#             dis = euclidean_distance(pointi, pointj)\n",
    "#             Gst.add_edge(pointi , pointj , weight=dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gst = nx.minimum_spanning_tree(Gst, algorithm=\"prim\", weight=\"weight\")\n",
    "# gst_weight = sum(data[\"weight\"] for u, v, data in Gst.edges(data=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prim_end_time = time.time()\n",
    "prim_elapsed_time = prim_end_time - prim_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "eprim_start_time = time.time()\n",
    "def calculate_distance(p1, p2):\n",
    "    x1, y1 = p1\n",
    "    x2, y2 = p2\n",
    "    return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "\n",
    "\n",
    "def prim_mst(points):\n",
    "    n = len(points)\n",
    "    visited = [False] * n\n",
    "    min_cost = [float(\"inf\")] * n\n",
    "    parent = [None] * n\n",
    "\n",
    "    min_cost[0] = 0\n",
    "    parent[0] = -1\n",
    "\n",
    "    for _ in range(n):\n",
    "        min_index = -1\n",
    "        for i in range(n):\n",
    "            if not visited[i] and (\n",
    "                min_index == -1 or min_cost[i] < min_cost[min_index]\n",
    "            ):\n",
    "                min_index = i\n",
    "\n",
    "        visited[min_index] = True\n",
    "\n",
    "        for i in range(n):\n",
    "            if not visited[i]:\n",
    "                distance = calculate_distance(points[min_index], points[i])\n",
    "                if distance < min_cost[i]:\n",
    "                    min_cost[i] = distance\n",
    "                    parent[i] = min_index\n",
    "\n",
    "    mst_edges = []\n",
    "    for i in range(1, n):\n",
    "        mst_edges.append((parent[i], i))\n",
    "\n",
    "    return mst_edges\n",
    "\n",
    "\n",
    "mst = prim_mst(points)\n",
    "eprim_end_time = time.time()\n",
    "eprim_elapsed_time = eprim_end_time - eprim_start_time\n",
    "print(\"MST Weight: \", eprim_wt := sum([calculate_distance(points[u], points[v]) for u, v in mst]))\n",
    "print(\"time taken by dcran : \", eprim_elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speedup = eprim_elapsed_time / dcran_elapsed_time\n",
    "print(f\"Speedup: {speedup:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_error = abs(dc_weight - eprim_wt) / eprim_wt * 100\n",
    "print(f\"Weight Error: {wt_error}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmst_start_time = time.time()\n",
    "def euclidean_distance(p1, p2):\n",
    "    return sum((a - b) ** 2 for a, b in zip(p1, p2)) ** 0.5\n",
    "\n",
    "\n",
    "def detect_connecting_edge(cluster1, cluster2):\n",
    "    center1 = [sum(x) / len(cluster1) for x in zip(*cluster1)]\n",
    "    center2 = [sum(x) / len(cluster2) for x in zip(*cluster2)]\n",
    "\n",
    "    min_dist1 = float(\"inf\")\n",
    "    min_point1 = None\n",
    "    for point in cluster1:\n",
    "        dist = euclidean_distance(point, center2)\n",
    "        if dist < min_dist1:\n",
    "            min_dist1 = dist\n",
    "            min_point1 = point\n",
    "\n",
    "    min_dist2 = float(\"inf\")\n",
    "    min_point2 = None\n",
    "    for point in cluster2:\n",
    "        dist = euclidean_distance(point, center1)\n",
    "        if dist < min_dist2:\n",
    "            min_dist2 = dist\n",
    "            min_point2 = point\n",
    "\n",
    "    return min_point1, min_point2\n",
    "\n",
    "\n",
    "def fast_mst(points):\n",
    "    n = len(points)\n",
    "    k = int(n**0.5)\n",
    "\n",
    "    # Divide-and-conquer stage\n",
    "    kmeans = KMeans(n_clusters=k).fit(points)\n",
    "    labels = kmeans.labels_\n",
    "    centers = kmeans.cluster_centers_.tolist()\n",
    "\n",
    "    mst_edges = []\n",
    "    total_weight = 0\n",
    "\n",
    "    for i in range(k):\n",
    "        cluster_points = [points[j] for j in range(n) if labels[j] == i]\n",
    "        pairwise_distances = squareform(pdist(cluster_points))\n",
    "        mst = minimum_spanning_tree(pairwise_distances).toarray().tolist()\n",
    "        edges = [\n",
    "            (u, v)\n",
    "            for u in range(len(cluster_points))\n",
    "            for v in range(u + 1, len(cluster_points))\n",
    "            if mst[u][v] > 0\n",
    "        ]\n",
    "        for u, v in edges:\n",
    "            weight = euclidean_distance(cluster_points[u], cluster_points[v])\n",
    "            mst_edges.append((cluster_points[u], cluster_points[v]))\n",
    "            total_weight += weight\n",
    "\n",
    "    pairwise_distances_centers = squareform(pdist(centers))\n",
    "    mst_centers = minimum_spanning_tree(pairwise_distances_centers).toarray().tolist()\n",
    "    center_edges = [\n",
    "        (i, j) for i in range(k) for j in range(i + 1, k) if mst_centers[i][j] > 0\n",
    "    ]\n",
    "\n",
    "    # Refinement stage\n",
    "    midpoints = []\n",
    "    for i, j in center_edges:\n",
    "        cluster1 = [points[m] for m in range(n) if labels[m] == i]\n",
    "        cluster2 = [points[m] for m in range(n) if labels[m] == j]\n",
    "        u, v = detect_connecting_edge(cluster1, cluster2)\n",
    "        mst_edges.append((u, v))\n",
    "        midpoint = [(a + b) / 2 for a, b in zip(u, v)]\n",
    "        midpoints.append(midpoint)\n",
    "        weight = euclidean_distance(u, v)\n",
    "        total_weight += weight\n",
    "\n",
    "    kmeans_refine = KMeans(n_clusters=len(midpoints), init=midpoints, n_init=1).fit(\n",
    "        points\n",
    "    )\n",
    "    labels_refine = kmeans_refine.labels_\n",
    "\n",
    "    for i in range(len(midpoints)):\n",
    "        cluster_points = [points[j] for j in range(n) if labels_refine[j] == i]\n",
    "        pairwise_distances = squareform(pdist(cluster_points))\n",
    "        mst = minimum_spanning_tree(pairwise_distances).toarray().tolist()\n",
    "        edges = [\n",
    "            (u, v)\n",
    "            for u in range(len(cluster_points))\n",
    "            for v in range(u + 1, len(cluster_points))\n",
    "            if mst[u][v] > 0\n",
    "        ]\n",
    "        for u, v in edges:\n",
    "            weight = euclidean_distance(cluster_points[u], cluster_points[v])\n",
    "            mst_edges.append((cluster_points[u], cluster_points[v]))\n",
    "            total_weight += weight\n",
    "\n",
    "    return mst_edges, total_weight\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n",
    "mst_edges, fmst_weight = fast_mst(points)\n",
    "fmst_end_time = time.time()\n",
    "fmst_edgecount = len(mst_edges)\n",
    "fmst_elapsed_time = fmst_end_time - fmst_start_time\n",
    "print(\"time taken by fast mst : \", fmst_elapsed_time)\n",
    "\n",
    "print(\"Total weight of the MST:\", fmst_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(savefile, \"r\") as f:\n",
    "    loaded_data = json.load(f)\n",
    "print(loaded_data)\n",
    "currres = []\n",
    "loaded_data.append(\n",
    "    [\n",
    "        points_count,\n",
    "        no_centres,\n",
    "        dc_weight,\n",
    "        fmst_weight,\n",
    "        eprim_wt,\n",
    "        dc_edgecount,\n",
    "        fmst_edgecount,\n",
    "        abs(fmst_weight - eprim_wt) / eprim_wt * 100,\n",
    "        abs(dc_weight - eprim_wt) / eprim_wt * 100,\n",
    "        dcran_elapsed_time,\n",
    "        fmst_elapsed_time,\n",
    "        eprim_elapsed_time,\n",
    "        eprim_elapsed_time / fmst_elapsed_time,\n",
    "        eprim_elapsed_time / dcran_elapsed_time,\n",
    "    ]\n",
    ")\n",
    "# Save the updated dictionary back to the JSON file\n",
    "with open(savefile, \"r\") as f:\n",
    "    loaded_data = json.load(f)\n",
    "print(loaded_data)\n",
    "currres = []\n",
    "loaded_data.append(\n",
    "    [\n",
    "        points_count,\n",
    "        no_centres,\n",
    "        dc_weight,\n",
    "        fmst_weight,\n",
    "        eprim_wt,\n",
    "        len(mst_edges),\n",
    "        len(mst),\n",
    "        abs(fmst_weight - eprim_wt) / eprim_wt * 100,\n",
    "        abs(dc_weight - eprim_wt) / eprim_wt * 100,\n",
    "        dcran_elapsed_time,\n",
    "        fmst_elapsed_time,\n",
    "        eprim_elapsed_time,\n",
    "        eprim_elapsed_time / fmst_elapsed_time,\n",
    "        eprim_elapsed_time / dcran_elapsed_time,\n",
    "    ]\n",
    ")\n",
    "# Save the updated dictionary back to the JSON file\n",
    "with open(savefile, \"w\") as f:\n",
    "    json.dump(loaded_data, f)\n",
    "\n",
    "headers = [\n",
    "    \"Points\",\n",
    "    \"Centres\",\n",
    "    \"DCRAN Wt\",\n",
    "    \"FMST Wt\",\n",
    "    \"Prim's Wt\",\n",
    "    \"FMST Edges\",\n",
    "    \"Prim's Edges\",\n",
    "    \"FMST Acc(%)\",\n",
    "    \"DCRAN Acc(%)\",\n",
    "    \"DCRAN Time (s)\",\n",
    "    \"FMST Time (s)\",\n",
    "    \"Prim's Time (s)\",\n",
    "    \"Prim's Speedup\",\n",
    "    \"DCRAN Speedup\",\n",
    "]\n",
    "# Format the data as a table using tabulate\n",
    "table_str = tabulate(\n",
    "    loaded_data[-15:],\n",
    "    headers,\n",
    "    tablefmt=\"pipe\",\n",
    "    floatfmt=(\n",
    "        \".0f\",\n",
    "        \".0f\",\n",
    "        \".1f\",\n",
    "        \".1f\",\n",
    "        \".1f\",\n",
    "        \".0f\",\n",
    "        \".0f\",\n",
    "        \".2f\",\n",
    "        \".2f\",\n",
    "        \".2f\",\n",
    "        \".2f\",\n",
    "        \".2f\",\n",
    "        \".2f\",\n",
    "        \".2f\",\n",
    "    ),\n",
    ")\n",
    "print(table_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
